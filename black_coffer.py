# -*- coding: utf-8 -*-
"""Black_coffer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hiTQc8ydpZ6L-4VSONmyaLoAqeHsD9oG
"""

# installing trafilatura library
!pip install goose3

# Importing necessary libraries
import pandas as pd
import numpy as np
import spacy
from goose3 import Goose # for text extraction from URL
import warnings
warnings.simplefilter("ignore")

"""# 1. Data Preparation"""

# Storing the Input file which contains all the URLs in a dataframe.
df= pd.read_excel("Input.xlsx")
# viewing top 5 values
df.head()

# storing the neg and pos words into respective dataframes from Master dictionary for text analysis 
negative_list = pd.read_csv('negative-words.txt', sep = '\n', encoding='iso-8859-1', header = None )
positive_list = pd.read_csv('positive-words.txt', sep = '\n', encoding='iso-8859-1', header = None )

# function to be used to loop each url and extract the text
def extract_text_web(url):
  g = Goose()
  article = g.extract(url=url)
  # process/store ...
  return article.cleaned_text

# storing the extracted text into a dedicated column
df['extracted_text']= [extract_text_web(url) for url in df['URL']]

# checking whether everything is stored nicely
df.head()

"""## **2. Text Analysis**

### **2.1 Sentimental Analysis**

#### **2.1.1 Cleaning using Stop Words Lists**
"""

# Cleaning using stop words lists
# adding all the txt files in the stopwords folder into one 
import glob
with open('stopwords.csv', 'a') as csv_file:
    for path in glob.glob('/content/stopwords*/*.txt'):
        with open(path, encoding = 'iso-8859-1') as txt_file:
            txt = txt_file.read() + '\n'
            csv_file.write(txt)

# creating a list of stopwords
stopwords = pd.read_csv("stopwords.csv", sep = '\n', header = None)
# as the stopwords are all in uppercase we convert it to lowercase and store it in a list
stopwords = stopwords[0].str.lower().to_list()

# function for removing stopwords
def clean_text(text):
  cleaned_text = " ".join([word for word in text.split() if word.lower() not in (stopwords)])
  return cleaned_text

# removing stopwords from the text stored in extracted_text column adn storing it in a new column
df['cleaned_text'] = [clean_text(text) for text in df['extracted_text']]

"""#### **2.1.2 Creating a dictionary of positive and negative words**"""

# As Instructed, adding only those words in the dictionary which are not found in stopwords
negative_words = [word for word in negative_list[0] if word not in (stopwords)]
positive_words = [word for word in positive_list[0] if word not in (stopwords)]

# creating the positive and neagtive dictionaries from the filtered list above
negative_words = {i:negative_words[i] for i in range(len(negative_words))}
positive_words = {i:positive_words[i] for i in range(len(positive_words))}

"""#### **2.1.3 Extracting derived variables**"""

# importing the RegexpTokenizer module from nltk library for selecting only the alphanumerix characters as tokens
# downloading the stopwords from nltk corpus
nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w+')

# creating a column with tokens of the corresponding text in cleaned_text column
df['tokens'] = [tokenizer.tokenize(text) for text in df['cleaned_text']]

# for positive_score calculation
def pos_score(tokens):
  '''since every word in positive_words is in 
  lowercase we need to the same for token'''
  positive_score = 0
  for token in tokens:
    if (token.lower() in positive_words.values()):
      positive_score +=1
  return positive_score

# for negative score calculation
def neg_score(tokens):
  '''since every word in negative_words
   is in lowercase we need to the same for token'''
  negative_score = 0
  for token in tokens:
    if (token.lower() in negative_words.values()):
      negative_score -=1
  return negative_score

# storing the positive and negative scores in their respective columns
df['positive_score'] = [pos_score(tokens) for tokens in df['tokens']]
df['negative_score'] = [neg_score(tokens) for tokens in df['tokens']]

"""The other two derived variables are calculated using the positive and negative score <br>
**Polarity Score**: This is the score that determines if a given text is positive or negative in nature. It is calculated by using the formula:<br> 
Polarity Score = (Positive Score â€“ Negative Score)/ ((Positive Score + Negative Score) + 0.000001)
Range is from -1 to +1 <br>
**Subjectivity Score**: This is the score that determines if a given text is objective or subjective. It is calculated by using the formula: <br>
Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)
Range is from 0 to +1

"""

# calculating polarity score and storing it in a dedicated column
def pol_score(i):
  polarity_score = ((df['positive_score'][i])-(df['negative_score'][i]))/((df['positive_score'][i]+df['negative_score'][i]) + 0.000001)
  return polarity_score
# adding the polarity score calculated to respective column
df['polarity_score'] = [pol_score(i) for i in range(len(df))]

# calculating subjectivity score and storing it in a dedicated column
def sub_score(i):
  subjectivity_score = (df['positive_score'][i]+df['negative_score'][i])/((len(df['cleaned_text'][i].split())) + 0.000001)
  return subjectivity_score
# adding the subjectivity score calculated to respective column
df['subjectivity_score'] = [sub_score(i) for i in range(len(df))]

# The polarity score and the subjectivity score are not in desired range so we will scale them appropriately
from sklearn.preprocessing import MinMaxScaler
# for polarity_score the range need to be -1 to +1
scaler = MinMaxScaler(feature_range=(-1, 1)) 
df['polarity_score'] = scaler.fit_transform(df['polarity_score'].values.reshape(-1, 1))

# for subjectivity_score the range needs to be 0 to 1
scaler = MinMaxScaler(feature_range=(0, 1))
df['subjectivity_score'] = scaler.fit_transform(df['subjectivity_score'].values.reshape(-1, 1))

# verifying the scaled data
print("max polarity score = ", df['polarity_score'].max())
print("min polarity score = " ,df['polarity_score'].min())
print("max subjectivity score = ", df['subjectivity_score'].max())
print("min subjectivity score = ", df['subjectivity_score'].min())

"""## **3. Analysis of Readability**

1. Average Sentence Length = the number of words/ the number of sentences.
2. Percentage of complex words = the number of complex words/ the number of words.
3. Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words.
"""

!pip install textstat

# functions needed for further analysis
from textstat.textstat import textstatistics
# for extracting sentences from text
def break_sentences(text):
    nlp = spacy.load('en_core_web_sm')
    doc = nlp(text)
    return list(doc.sents)


# for counting syllables 
def syllables_count(word):
    return textstatistics().syllable_count(word)
  


# for calculating word count
def word_count(text):
    stop_words = set(stopwords.words('english'))
    words = tokenizer.tokenize(text)
    filtered_words = []
    for word in words:
      if word not in stop_words:
        filtered_words.append(word)
    return len(filtered_words)

# for counting words per sentence
def word_count_sent(text):
    sentences = break_sentences(text)
    words = 0
    for sentence in sentences:
        words += len([token for token in sentence])
    return words


# for calculating average sentence length 
def num_sentence(sent):
  sentences= nltk.sent_tokenize(sent)
  sent_total = len(sentences)
  words_total = word_count(sent)
  avg_sent_len =words_total/sent_total
  return avg_sent_len


# for calculating Average Number of words per sentence
def avg_words_sent(sent):
  sentences= nltk.sent_tokenize(sent)
  sent_total = len(sentences)
  words_total = word_count_sent(sent)
  avg_words_sent_len = words_total/sent_total
  return avg_words_sent_len


# for calculating total number of Complex words
def difficult_words(text):
  # difficult words are those with syllables > 2
  nlp = spacy.load('en_core_web_sm')
  doc = nlp(text)
  # Find all words in the text
  words = []
  sentences = break_sentences(text)
  for sentence in sentences:
    words += [str(token) for token in sentence]
  
  diff_words_set = set()
    
  for word in words:
    syllable_count = syllables_count(word)
    if word not in nlp.Defaults.stop_words and syllable_count > 2:
        diff_words_set.add(word)

  return len(diff_words_set)


# for calculating percentage of complex words
def perc_comp_words(text):
  complex_word_count = difficult_words(text)
  words_count = word_count(text)
  
  perc_complex_words = complex_word_count/words_count
  return (perc_complex_words*100)

"""#### **Adding the columns Average_Sentence_length, Percentage_of_complex_words and fog_index**"""

# Adding the columns Average_Sentence_Length and Percentage_of_complex_words

df['Average_Sentence_Length'] = [num_sentence(sent) for sent in df['extracted_text']]

df['Percentage_of_complex_words'] = [perc_comp_words(text) for text in df['extracted_text']]

# Adding the Fog Index column
def fog_index(i):
  fog_ind = 0.4*(df['Average_Sentence_Length'][i] + df['Percentage_of_complex_words'][i])
  return fog_ind
df['Fog_index'] = [fog_index(i) for i in range(len(df))]

"""### **4. Adding the remaining columns, Average Number of Words Per Sentence, Complex_word_count, word_count, syllable_count_per_word, Personal_pronouns and average_word_length**"""

# Adding Average_number_of_words_per_sentences in respective column.
df['Average_number_of_words_per_sentence'] = [avg_words_sent(sent) for sent in df['extracted_text']]

# Adding the remaining data in respective columns
df['complex_word_count'] = [difficult_words(text) for text in df['extracted_text']]

df['word_count'] = [word_count(text) for text in df['extracted_text']]

def syllable_count_per_word(words):
  for word in words:
    syllable_count = syllables_count(word)
  return syllable_count

df['syllable_count'] = [syllable_count_per_word(words) for words in df['extracted_text']]

# Adding last two columns to dataframe
import re
def count_personal_pronouns(text):
  pronoun_count = re.compile(r'\b(I|we|ours|my|mine|(?-i:us))\b', re.I)
  pronouns = pronoun_count.findall(text)
  return len(pronouns)
# adding the Personal_pronouns column
df['Personal_pronouns'] = [count_personal_pronouns(text) for text in df['extracted_text']]

# adding the last column Average_word_length

def average_word_length(text):
  stop_words = set(stopwords.words('english'))
  words = tokenizer.tokenize(text)
  filtered_chars = []
  for word in words:
    if word not in stop_words:
      filtered_chars += [char for char in word]
  
  char_count = len(filtered_chars)
  avg_words_len = char_count/len(words)
  return round(avg_words_len)

df['Average_word_length'] = [average_word_length(words) for words in df['extracted_text']]

"""### **Storing the Output in output.csv as given in the output data structure**"""

df2 = pd.read_excel("Output Data Structure.xlsx")
col_list = list(df2.columns)

# dropping the columns not in output data structure
df.drop(['extracted_text', 'cleaned_text', 'tokens'], axis = 1, inplace = True)

col_dict = {'positive_score':'POSITIVE SCORE', 'negative_score':'NEGATIVE SCORE','polarity_score':'POLARITY SCORE',
 'subjectivity_score':'SUBJECTIVITY SCORE',
 'Average_Sentence_Length':'AVG SENTENCE LENGTH',
 'Percentage_of_complex_words':'PERCENTAGE OF COMPLEX WORDS',
 'Fog_index':'FOG INDEX',
 'Average_number_of_words_per_sentence':'AVG NUMBER OF WORDS PER SENTENCE',
 'complex_word_count':'COMPLEX WORD COUNT',
 'word_count':'WORD COUNT',
 'syllable_count':'SYLLABLE PER WORD',
 'Personal_pronouns':'PERSONAL PRONOUNS',
 'Average_word_length':'AVG WORD LENGTH'}

# renaming the columns in df as per the output data structure

df.rename(columns = col_dict, inplace = True)

# Storing the df in output.csv for submission.
df.to_csv('Output.csv')